#!/usr/bin/env python3
"""
Scrape a specific tweet by URL and save it
"""

import asyncio
import json
import os
from datetime import datetime
from twscrape import API
from dotenv import load_dotenv

load_dotenv()

async def scrape_tweet_by_url(url: str, user_hashtags: list = None):
    api = API()
    
    # Extract tweet ID from URL
    tweet_id = int(url.split('/')[-1].split('?')[0])
    
    try:
        print(f"Scraping tweet ID: {tweet_id}")
        tweet = await api.tweet_details(tweet_id)
        
        if tweet:
            # Extract media if available
            media_data = []
            print(f"🔍 Checking for media in tweet...")
            print(f"   Tweet has 'media' attribute: {hasattr(tweet, 'media')}")
            
            if hasattr(tweet, 'media'):
                print(f"   tweet.media value: {tweet.media}")
                print(f"   tweet.media type: {type(tweet.media)}")
                
                if tweet.media:
                    if hasattr(tweet.media, 'photos'):
                        media_count = len(tweet.media.photos) + len(getattr(tweet.media, 'videos', [])) + len(getattr(tweet.media, 'animated', []))
                        print(f"   Found {media_count} total media items")
                        print(f"     Photos: {len(tweet.media.photos)}")
                        print(f"     Videos: {len(getattr(tweet.media, 'videos', []))}")
                        print(f"     Animated: {len(getattr(tweet.media, 'animated', []))}")
                    else:
                        print("   Media object structure unknown")
                    # Process photos
                    for i, media_item in enumerate(tweet.media.photos):
                        print(f"   Photo {i+1}:")
                        print(f"     Type: {getattr(media_item, 'type', 'unknown')}")
                        print(f"     Has variants: {hasattr(media_item, 'variants')}")
                        print(f"     Has url: {hasattr(media_item, 'url')}")
                        
                        media_url = None
                        if hasattr(media_item, 'variants') and media_item.variants:
                            print(f"     Variants count: {len(media_item.variants)}")
                            # Find the highest bitrate variant for videos
                            best_variant = max(media_item.variants, key=lambda v: getattr(v, "bitrate", 0) if getattr(v, "bitrate", None) else 0)
                            media_url = getattr(best_variant, "url", None)
                            print(f"     Best variant URL: {media_url}")
                        elif hasattr(media_item, 'url'):
                            media_url = media_item.url
                            print(f"     Direct URL: {media_url}")
                        
                        if media_url:
                            media_info = {
                                'url': media_url,
                                'type': 'photo',  # This is from photos array
                                'width': getattr(media_item, 'width', None),
                                'height': getattr(media_item, 'height', None)
                            }
                            media_data.append(media_info)
                            print(f"     Added to media_data: {media_info}")
                        else:
                            print(f"     No valid URL found for this photo")
                    
                    # Process videos
                    for i, media_item in enumerate(getattr(tweet.media, 'videos', [])):
                        print(f"   Video {i+1}:")
                        print(f"     Has variants: {hasattr(media_item, 'variants')}")
                        
                        media_url = None
                        if hasattr(media_item, 'variants') and media_item.variants:
                            print(f"     Variants count: {len(media_item.variants)}")
                            best_variant = max(media_item.variants, key=lambda v: getattr(v, "bitrate", 0) if getattr(v, "bitrate", None) else 0)
                            media_url = getattr(best_variant, "url", None)
                            print(f"     Best variant URL: {media_url}")
                        
                        if media_url:
                            media_info = {
                                'url': media_url,
                                'type': 'video',
                                'width': getattr(media_item, 'width', None),
                                'height': getattr(media_item, 'height', None)
                            }
                            media_data.append(media_info)
                            print(f"     Added to media_data: {media_info}")
                    
                    # Process animated GIFs
                    for i, media_item in enumerate(getattr(tweet.media, 'animated', [])):
                        print(f"   Animated GIF {i+1}:")
                        media_url = getattr(media_item, 'url', None)
                        if media_url:
                            media_info = {
                                'url': media_url,
                                'type': 'animated_gif',
                                'width': getattr(media_item, 'width', None),
                                'height': getattr(media_item, 'height', None)
                            }
                            media_data.append(media_info)
                            print(f"     Added to media_data: {media_info}")
                else:
                    print("   tweet.media is empty or falsy")
            else:
                print("   Tweet has no 'media' attribute")
            
            print(f"📊 Final media_data: {media_data}")
            
            tweet_data = {
                'id': tweet.id,
                'text': tweet.rawContent,
                'author': tweet.user.username,
                'author_name': tweet.user.displayname,
                'author_followers': tweet.user.followersCount,
                'author_verified': tweet.user.verified,
                'created_at': str(tweet.date),
                'retweet_count': tweet.retweetCount,
                'like_count': tweet.likeCount,
                'reply_count': tweet.replyCount,
                'quote_count': tweet.quoteCount,
                'view_count': getattr(tweet, 'viewCount', None),
                'url': f"https://x.com/{tweet.user.username}/status/{tweet.id}",
                'media': media_data, # Add media data to tweet
                'scraped_at': str(datetime.now())
            }
            
            print("✅ Tweet scraped successfully!")
            print(f"📝 Author: @{tweet_data['author']} ({tweet_data['author_name']})")
            print(f"📅 Date: {tweet_data['created_at']}")
            print(f"💬 Text: {tweet_data['text']}")
            print(f"👍 Likes: {tweet_data['like_count']}")
            print(f"🔄 Retweets: {tweet_data['retweet_count']}")
            print(f"💬 Replies: {tweet_data['reply_count']}")
            if tweet_data['media']:
                print(f"📷 Media files: {len(tweet_data['media'])}")
                for i, media in enumerate(tweet_data['media']):
                    print(f"   {i+1}. {media['type']}: {media['url']}")
            
            # Save to JSON
            filename = f"tweet_{tweet_id}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(tweet_data, f, indent=2, ensure_ascii=False)
            
            print(f"\n💾 Tweet saved to: {filename}")
            return tweet_data
            
        else:
            print("❌ Tweet not found or not accessible")
            return None
            
    except Exception as e:
        print(f"❌ Error scraping tweet: {e}")
        return None

if __name__ == "__main__":
    # Get URL from command line argument or use default
    import sys
    
    if len(sys.argv) > 1:
        tweet_url = sys.argv[1]
    else:
        tweet_url = input("Enter tweet URL: ")
    
    result = asyncio.run(scrape_tweet_by_url(tweet_url))
    if result:
        print("\n🎉 Scraping completed successfully!")
    else:
        print("\n💥 Scraping failed!")